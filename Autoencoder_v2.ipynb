{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an autoencoder for dimensionaltiy reduction\n",
    "# \n",
    "# Model attributes: use dataset API to avoid feed dict\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset as Ds\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1078, 26596)\n"
     ]
    }
   ],
   "source": [
    "data  =  pd.read_csv('/Users/dawnstear/desktop/chop_cellpred/data.csv')  \n",
    "print(np.shape(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "data = shuffle(data)\n",
    "celltypes = data['TYPE'] # save cell type vector in case we need it later\n",
    "labels = data['Labels'] # save labels\n",
    "data_ = data.drop(['Labels','TYPE'],axis=1) # Take off types & labels for input (AE is unsupervised) \n",
    "\n",
    "cellcount, genecount = np.shape(data_)\n",
    "BUFFER = 55 # .shuffle(BUFFER), already shuffled\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_.values,labels.values,test_size=0.2,random_state=144)\n",
    "\n",
    "# Create dataset to avoid using feed_dict() (its very slow) \n",
    "train_dataset = Ds.from_tensor_slices((X_train)).repeat().batch(BATCH_SIZE)\n",
    "test_dataset = Ds.from_tensor_slices((X_test)).repeat().batch(BATCH_SIZE)\n",
    "\n",
    "# Create general iterator, seamlessly switch bt train data and test data sets\n",
    "iterator = tf.data.Iterator.from_structure(train_dataset.output_types,train_dataset.output_shapes)\n",
    "\n",
    "# This will return a tuple where next_element[0] = data, if we have labels [via .zip],  next_element[1] = labels\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# Make datasets that we can initialize separately, but using the same structure via the common iterator\n",
    "training_init_op = iterator.make_initializer(train_dataset)\n",
    "testing_init_op = iterator.make_initializer(test_dataset)\n",
    "\n",
    "# do we need to normalize/regularize or do batch correction ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla AUTOENCODER model adapted from: \n",
    "#   Author: Aymeric Damien\n",
    "#   Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.01\n",
    "num_steps = 30000\n",
    "batch_size = 256\n",
    "display_step = 1000\n",
    "examples_to_show = 10\n",
    "\n",
    "# Network Parameters\n",
    "num_hidden_1 = 256 # 1st layer num features\n",
    "num_hidden_2 = 128 # 2nd layer num features (the latent space aka # of dimensions we've reduced to)\n",
    "num_input =  genecount # number of features per cell sample \n",
    "\n",
    "\n",
    "# Define weights & biases\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([num_input])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the encoder\n",
    "def encoder(expression_matrix): # what to add to this fcn?\n",
    "    # Perform \"exponential linear unit\"  activation fcn on X*W + b\n",
    "    layer_1 = tf.nn.elu(tf.add(tf.matmul(expression_matrix, weights['encoder_h1']), biases['encoder_b1']))\n",
    "    layer_2 = tf.nn.elu(tf.add(tf.matmul(layer_1, weights['encoder_h2']),biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(latent_space):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(latent_space, weights['decoder_h1']),biases['decoder_b1']))\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(tf.cast(next_element,tf.float32))  # cast expression matrix to float32\n",
    "decoder_op = decoder(encoder_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = tf.cast(next_element,tf.float32)\n",
    "\n",
    "# Define loss and optimizer, minimize the  mean squared error\n",
    "loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    # Run Data.dataset iterator initializer\n",
    "    sess.run(training_init_op)   # reset weights each time ? if train flag == 1\n",
    "\n",
    "    # Training\n",
    "    for i in range(1, num_steps+1):\n",
    "        # Prepare Data\n",
    "        # Get the next batch of MNIST data (only images are needed, not labels)\n",
    "        # batch_x, _ = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        _, l = sess.run([optimizer, loss], feed_dict={X: batch_x})\n",
    "        # Display logs per step\n",
    "        if i % display_step == 0 or i == 1:\n",
    "            print('Step %i: Minibatch Loss: %f' % (i, l))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
